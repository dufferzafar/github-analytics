{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.types as spark_types\n",
    "\n",
    "import utils\n",
    "\n",
    "spark = SparkSession.builder.master(\"spark://vm1:7077\").appName(\"Cluster Code - Zafar\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+------------+----------+-------------------+\n",
      "| id|author_id|committer_id|project_id|         created_at|\n",
      "+---+---------+------------+----------+-------------------+\n",
      "|  1|        1|           1|         1|2012-08-01 20:33:21|\n",
      "|  2|        2|           2|         1|2012-08-01 13:25:36|\n",
      "|  3|        2|           2|         1|2012-06-18 03:39:30|\n",
      "|  4|        2|           2|         1|2012-06-11 07:47:16|\n",
      "|  5|        2|           2|         1|2012-06-11 07:45:07|\n",
      "|  6|        2|           2|         1|2012-05-07 06:00:56|\n",
      "|  7|        2|           2|         1|2012-03-08 04:47:19|\n",
      "|  8|        2|           2|         1|2012-03-08 04:40:43|\n",
      "|  9|        2|           2|         1|2012-03-08 04:40:25|\n",
      "| 10|        2|           2|         1|2012-03-08 04:24:22|\n",
      "+---+---------+------------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing to see if everything is okay\n",
    "commits = utils.read_csv(spark, \"hdfs:/commits_new.csv\")\n",
    "commits.limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of source & fork repositories of users (shouldn't be run again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects =  utils.read_csv(spark, \"hdfs:/projects.csv\", \"projects_new.csv\")\n",
    "\n",
    "# This file has now been replaced with a new version\n",
    "user_more =  utils.read_csv(spark, \"hdfs:/user_more.csv\")\n",
    "\n",
    "# Find source repos\n",
    "df4 = (\n",
    "    projects\n",
    "    .where((projects.deleted == 0) & (projects.forked_from.isNull()))\n",
    "    .groupby(\"owner_id\")\n",
    "    .count()\n",
    "    .withColumnRenamed(\"count\", \"repos_source\")\n",
    "    .withColumnRenamed(\"owner_id\", \"user_id\")\n",
    ")\n",
    "\n",
    "# Find forks\n",
    "df6 = (\n",
    "    projects\n",
    "    .where((projects.deleted == 0) & (projects.forked_from.isNotNull()))\n",
    "    .groupby(\"owner_id\")\n",
    "    .count()\n",
    "    .withColumnRenamed(\"count\", \"repos_forks\")\n",
    "    .withColumnRenamed(\"owner_id\", \"user_id\")\n",
    ")\n",
    "\n",
    "# Join Data\n",
    "df5 = user_more.join(df4, \"user_id\", \"full\").join(df6, \"user_id\", \"full\")\n",
    "\n",
    "# Write to local directorya\n",
    "df5.write.csv(\n",
    "    \"/user_more_2\",\n",
    "    mode=\"overwrite\",\n",
    "    nullValue=\"\\\\N\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue Punchcard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+----+--------+\n",
      "|year|month| day|hour|   count|\n",
      "+----+-----+----+----+--------+\n",
      "|null| null|null|null|54086297|\n",
      "+----+-----+----+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "issues = utils.read_csv(spark, \"hdfs:/issues.csv\")\n",
    "\n",
    "df5 = (\n",
    "    issues\n",
    "    .where(\n",
    "        issues.created_at.isNotNull()\n",
    "    )\n",
    "    .select(\n",
    "        F.year('created_at').alias('year'), \n",
    "        F.month('created_at').alias('month'), \n",
    "        F.dayofmonth('created_at').alias('day'), \n",
    "        F.hour('created_at').alias('hour')\n",
    "    )\n",
    "   .groupBy('year', 'month', 'day', 'hour')\n",
    "   .count()\n",
    ")\n",
    "\n",
    "df5.limit(10).show()\n",
    "\n",
    "# df5.explain()\n",
    "# df5.coalesce(1).write.json(\"hdfs:/issue_punchcard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Commits of each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "commits = utils.read_csv(spark, \"hdfs:/commits_new.csv\")\n",
    "\n",
    "res = (\n",
    "    commits\n",
    "    .groupby(\"author_id\")\n",
    "    .count()\n",
    "    .withColumnRenamed(\"count\", \"commits_authored\")\n",
    "    .withColumnRenamed(\"author_id\", \"user_id\")\n",
    ")\n",
    "\n",
    "res.write.csv(\n",
    "    \"hdfs:/user_commit_count\",\n",
    "    mode=\"overwrite\",\n",
    "    nullValue=\"\\\\N\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Commits of every project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# commits = utils.read_csv(spark, \"hdfs:/commits_new.csv\")\n",
    "\n",
    "res = (\n",
    "    commits\n",
    "    .groupby(\"project_id\")\n",
    "    .count()\n",
    "    .withColumnRenamed(\"count\", \"total_commits\")\n",
    ")\n",
    "\n",
    "res.write.csv(\n",
    "    \"hdfs:/project_commit_count\",\n",
    "#     mode=\"overwrite\",\n",
    "    nullValue=\"\\\\N\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Commits on a project not authored by Owner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "commits = utils.read_csv(spark, \"hdfs:/commits_new.csv\")\n",
    "projects = utils.read_csv(spark, \"hdfs:/projects_new.csv\")\n",
    "\n",
    "commits.createOrReplaceTempView(\"commits\")\n",
    "projects.createOrReplaceTempView(\"projects\")\n",
    "\n",
    "q = \"\"\"\n",
    "    SELECT C.project_id as project_id, COUNT(*) as total_commits_by_others\n",
    "    FROM commits as C, projects as P\n",
    "    WHERE C.project_id = P.id\n",
    "    AND C.author_id <> P.owner_id\n",
    "    GROUP BY C.project_id\n",
    "\"\"\"\n",
    "\n",
    "res = spark.sql(q)\n",
    "\n",
    "res.write.csv(\n",
    "    \"hdfs:/project_commit_others_count\",\n",
    "#     mode=\"overwrite\",\n",
    "    nullValue=\"\\\\N\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of commits of a user made on repositories not owned by them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"\"\"\n",
    "    SELECT C.author_id as user_id, \n",
    "           COUNT(*) as total_commits_on_other_repos\n",
    "\n",
    "    FROM commits as C, projects as P\n",
    "\n",
    "    WHERE C.project_id = P.id\n",
    "    AND C.author_id <> P.owner_id\n",
    "\n",
    "    GROUP BY C.author_id\n",
    "\"\"\"\n",
    "\n",
    "res = spark.sql(q)\n",
    "\n",
    "res.write.csv(\n",
    "    \"hdfs:/user_commit_others_count\",\n",
    "#     mode=\"overwrite\",\n",
    "    nullValue=\"\\\\N\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Top\" Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+--------------------+---------+---------+-------+------------+-----------+---------+---------------+------+-----+-------+--------------+\n",
      "|user_id|        login|             company|following|followers|starred|repos_source|repos_forks|has_stars|contributess_to|issues|pulls|commits|commits_others|\n",
      "+-------+-------------+--------------------+---------+---------+-------+------------+-----------+---------+---------------+------+-----+-------+--------------+\n",
      "|   3871| sindresorhus|@avajs @chalk @ye...|      188|    15755|   2274|         974|        142|   226099|            565|  3080|  108|  26326|          8341|\n",
      "|6498757| FreeCodeCamp|                null|        1|      110|      6|          95|          5|   214583|           null|     1| null|     18|          null|\n",
      "| 376498|           Tj|                Apex|      175|    25827|    966|         238|         81|   114326|            138|   873| null|   4922|          3539|\n",
      "| 234594|       docker|                null|        5|        3|      8|         121|         10|   109060|              1|     1| null|     12|          null|\n",
      "|    896|  JakeWharton|        Square, Inc.|       24|    30161|    261|          86|         24|    82059|             58|  1125|  168|  16014|         11195|\n",
      "|    796| kennethreitz|              Heroku|      224|    13360|    919|         119|         37|    76984|            105|   473|   41|  12559|          6860|\n",
      "|   1199|     substack|    bits cooperative|      238|    11397|    277|         771|        195|    74744|            223|   291|   43|  19767|          4870|\n",
      "|  17717|       getify|    Getify Solutions|        9|     6360|    254|          44|          5|    73675|             15|   437|    4|   3116|           155|\n",
      "| 274322|          vhf|                null|       50|     2449|    531|          63|         69|    73353|             35|   135|    9|   5234|          1426|\n",
      "| 215522|     Yalantis|                null|        4|     null|      4|          65|          2|    68689|           null|  null| null|   null|          null|\n",
      "|   1954|  visionmedia|                null|      157|    11714|    624|         160|         63|    67825|             80|  2343|   40|  29853|         22294|\n",
      "|1123322|        Apple|                null|     null|        7|   null|          37|          1|    66930|           null|  null| null|   null|          null|\n",
      "|  15119| robbyrussell|   Planet Argon, LLC|       33|     1933|     50|           8|         26|    62104|             18|    39| null|   2108|          1122|\n",
      "| 667293|       nodejs|                null|     null|       21|   null|         158|          9|    59809|           null|  null| null|   null|          null|\n",
      "|3752431|  donnemartin|                null|        9|     1072|    976|          37|          5|    58763|              7|   176|    3|   4597|           458|\n",
      "| 293854|         Zeit|                null|        2|     null|   null|          64|          3|    57722|           null|  null| null|   null|          null|\n",
      "|   6240|   addyosmani|              Google|      243|    24604|    522|         147|        154|    55154|            308|  1195|   36|  10762|          6865|\n",
      "| 417948|      gaearon|            Facebook|      177|    13007|   1172|          59|        177|    53475|             69|  1079|   38|  10576|          5387|\n",
      "|2205410|kamranahmedse|             tajawal|       65|      177|    569|          43|         13|    50494|             16|    43| null|   1610|           187|\n",
      "|2813904|      jwasham|                null|        7|      805|     80|          31|          7|    49707|              1|    15| null|   1157|           149|\n",
      "|1147949|     wasabeef|    CyberAgent, Inc.|       41|     3341|    819|          22|          7|    49650|              6|     6| null|   1495|           153|\n",
      "|   3816|      daneden|            Facebook|      101|     3738|     61|          45|         10|    49553|             18|    30|    2|   2409|           459|\n",
      "|   7125|    jashkenas|The New York Time...|     null|    11410|      6|          17|       null|    49270|             24|    91|    1|  10622|          9589|\n",
      "|1963067|      tencent|                null|     null|       14|      5|          38|          2|    48844|           null|  null| null|      2|             2|\n",
      "|2629032|     typicode|                null|     null|     1014|    166|          29|         19|    47616|             11|    33|    1|   3626|            94|\n",
      "|   5260|      antirez|          Redis Labs|        2|     7215|     41|          54|          3|    47226|             13|   185| null|  12361|          5178|\n",
      "| 204125|      elastic|                null|        6|        9|     20|         187|         19|    46946|              6|  null| null|     19|             9|\n",
      "|   5203|     torvalds|    Linux Foundation|     null|    52722|      1|           5|          2|    46605|            105|     1| null|  48823|         39566|\n",
      "| 616741|       ruanyf|                null|     null|    14701|    124|          36|         12|    45361|             11|     4|    1|   4619|           390|\n",
      "|  13009|     mbostock|                null|       13|    14928|     29|          53|         18|    44739|             35|  1284|  137|  15904|         11370|\n",
      "|   2532|       mrdoob|                null|      110|     9651|     75|          32|         17|    44493|             31|   115|   11|  12837|          3234|\n",
      "|  10227|       feross|Study Notes, WebT...|      199|     5577|    854|         163|        153|    43445|            155|  1582|   13|  17300|          2083|\n",
      "| 129526|     bevacqua| JavaScript @elastic|      176|     2954|    339|         186|        102|    42447|             62|   201|    6|  14245|          3267|\n",
      "|   6258|      necolas|             Twitter|       27|     5799|    289|          31|         34|    42179|             70|  1079|   31|   4322|          2596|\n",
      "| 139710|    toddmotto|Telerik, Ultimate...|       43|     3272|    133|          59|         23|    41596|             37|    50|    6|   3454|          1319|\n",
      "|  10321|      blueimp|         blueimp.net|     null|     3237|     27|          52|          6|    40990|              9|    11|    1|   4155|          1839|\n",
      "|   2163|      hakimel|              Slides|       30|     7760|     54|          21|          3|    40870|             16|    42| null|   2216|           778|\n",
      "|   8293|        spf13|             @Google|       81|     2905|    572|          35|         54|    40784|             54|    68|    4|   1938|           466|\n",
      "|  43814|    justjavac|    @Flarum-Chinese |      129|     5685|    728|          34|         25|    40403|             13|   118| null|   3014|           415|\n",
      "| 490958|       jekyll|                null|     null|     null|   null|          53|       null|    40031|           null|  null| null|   null|          null|\n",
      "|2574738|        vinta|       @StreetVoice |      122|     1512|   1958|          26|         18|    39832|             10|   188| null|   2239|           194|\n",
      "|   3569|    mitsuhiko|                null|       21|     8801|     52|         138|         55|    38750|             50|   478|   15|  13938|          8735|\n",
      "| 122176|      astaxie|               Apple|       18|     6334|    100|          34|         40|    37919|             36|    62|    1|   5121|          2264|\n",
      "|1279090|        Realm|                null|     null|     null|   null|          31|          5|    37826|           null|  null| null|   null|          null|\n",
      "| 501475|        jlevy|                null|       87|      684|    885|           8|         13|    37551|              3|   181|    2|   1144|           391|\n",
      "|  11213| nicklockwood|     Charcoal Design|     null|     4179|     32|          64|         23|    37365|             20|    13|    2|   3519|          1750|\n",
      "|  76839|     Firebase|                null|        8|       39|     10|         147|         20|    37320|              1|  null| null|   null|          null|\n",
      "|   5823|        tpope|                null|       25|     9149|     19|          80|          9|    36680|             18|    55|   18|   5895|          2683|\n",
      "|   1570|      defunkt|            @github |      230|    18522|    139|          78|         29|    36487|             61|   170|    6|   5068|          4073|\n",
      "|3582041|     electron|                null|     null|     null|   null|          55|          1|    35206|           null|  null| null|      2|          null|\n",
      "+-------+-------------+--------------------+---------+---------+-------+------------+-----------+---------+---------------+------+-----+-------+--------------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_more = utils.read_csv(spark, \"hdfs:/users_more.csv\")\n",
    "users = utils.read_csv(spark, \"hdfs:/users.csv\")\n",
    "\n",
    "users = (\n",
    "    users\n",
    "    .where(\n",
    "          (users.type == \"USR\")\n",
    "        & (users.deleted == 0)\n",
    "        & (users.fake == 0)\n",
    "    )\n",
    "    .withColumnRenamed(\"id\", \"user_id\")\n",
    "    .select(\"user_id\", \"login\", \"company\")\n",
    ")\n",
    "\n",
    "user_new = users.join(users_more, \"user_id\", \"left\")\n",
    "\n",
    "top = (\n",
    "    user_new\n",
    "#     .orderBy(\"followers\", ascending=False)\n",
    "    .orderBy(\"has_stars\", ascending=False)\n",
    ")\n",
    "\n",
    "top.show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Top\" Users from India"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+--------------------+--------------+---------+---------+---------+--------------+------+-----+-------+--------------+\n",
      "| user_id|            login|             company|         state|     city|followers|has_stars|contributes_to|issues|pulls|commits|commits_others|\n",
      "+--------+-----------------+--------------------+--------------+---------+---------+---------+--------------+------+-----+-------+--------------+\n",
      "|   23402|          hemanth|             @paypal|          null|     null|     1209|    14032|           148|   505|   39|   5212|          1420|\n",
      "| 6310255|amitshekhariitbhu|          Bobble App|         Delhi|New Delhi|      311|    10360|             7|    10| null|   1581|           509|\n",
      "| 3408218|    sachinchoolur|Available for New...|     Karnataka|Bengaluru|      255|     9799|             3|     1|    1|    901|            53|\n",
      "|  163695|        chinchang|           @wingify |         Delhi|    Delhi|      468|     9326|             9|   136|    2|   1647|           245|\n",
      "| 5769465|          nisrulz|      @omnilabs-inc |       Haryana|  Gurgaon|      320|     8024|             1|    47|    2|   2788|           278|\n",
      "| 6559743|        aritraroy|                null|          null|     null|      189|     8005|          null|    32| null|    285|             5|\n",
      "| 1343504|        sameersbn|                null|           Goa|     null|      582|     7706|            23|    69|   10|  15917|          4680|\n",
      "| 6880792|   kailashahirwar|                null|     Karnataka|Bengaluru|        4|     7206|          null|  null| null|     45|            14|\n",
      "| 5107602|           iCHAIT|                null|          null|     null|      129|     7143|             5|    53|    8|   1363|           498|\n",
      "| 2600239|      dhamaniasad|                null|   Maharashtra|   Nagpur|       72|     7089|            39|    41| null|   1325|           173|\n",
      "| 5540963|          naman14|                null|         Delhi|New Delhi|      466|     6818|             5|     5|    1|   1320|           453|\n",
      "|   26271|         sitaramc|                null|Andhra Pradesh|Hyderabad|      335|     6306|             5|    12| null|   2171|           899|\n",
      "|   39721|    threepointone|                null|     Karnataka|Bengaluru|      346|     5445|            14|   167|    3|   2648|           194|\n",
      "|  300567|      siddharthkp|                null|     Karnataka|Bengaluru|       46|     5287|            13|   137|    1|   2398|           133|\n",
      "|  428875|       kovidgoyal|                null|   Maharashtra|   Mumbai|      244|     4978|             4|    62|   16|  29601|          3899|\n",
      "| 6131656|            adtac|                null|    Tamil Nadu|  Chennai|        9|     4866|             1|   209|    3|   2040|          1214|\n",
      "| 2940742|            jarun|                null|     Karnataka|Bengaluru|      117|     4462|             1|    64| null|   3455|            78|\n",
      "| 8541687|       metagrover|           TAKE ZERO|          null|     null|       65|     3998|             4|    61| null|   1466|          1431|\n",
      "| 2822668|            ghosh|           Cleartrip|     Karnataka|Bengaluru|      102|     3967|             9|    23|    1|   1104|           104|\n",
      "|  981089|         rishabhp|                null|          null|     null|      126|     3897|             1|    26| null|    536|           428|\n",
      "|30680352|      OmkarPathak|                null|   Maharashtra|     Pune|     null|     2968|          null|     5| null|    719|            13|\n",
      "| 2566115|           webkul|              WebKul| Uttar Pradesh|    Noida|       11|     2917|             3|     3| null|    126|             8|\n",
      "|   66055|  ragunathjawahar|        Mobs & Geeks|    Tamil Nadu|  Chennai|      317|     2899|            10|    33| null|    761|           244|\n",
      "|   74479|         HashNuke|                null|     Karnataka|Bengaluru|      276|     2765|            35|    92|    4|   4985|          1123|\n",
      "|11739754|          nitin42|                null|          null|Dehra Dun|       12|     2688|             1|    43|    1|   2475|            42|\n",
      "| 5644707|    princejwesley|@nodejs Core Coll...|    Tamil Nadu|  Chennai|      123|     2651|             7|    29|    1|   1191|           170|\n",
      "| 1897977|      ajinabraham|        OpenSecurity|     Karnataka|Bengaluru|      418|     2464|            21|    87|    1|    912|           238|\n",
      "| 8786853| developer-shivam|                null|         Delhi|New Delhi|       19|     2449|             4|  null| null|    434|            17|\n",
      "| 5787841|      Ashok-Varma|           ClearTrip|          null|     null|       55|     2437|          null|    18| null|    234|             2|\n",
      "| 8392485|          hanc00l|                null|        Odisha|  Cuttack|      162|     2335|          null|     3| null|     24|             3|\n",
      "| 2446188|          yask123|          @Flipkart |         Delhi|New Delhi|      324|     2173|             7|   101| null|   2833|            43|\n",
      "| 9555236|    farizrahman4u|         @datalogai |          null|     null|      125|     2172|             4|    23|    1|   1352|           315|\n",
      "|  274435|           satyan|                null|     Karnataka|Bengaluru|       84|     2153|             3|     8| null|    145|            53|\n",
      "| 7721759|   TakeoffAndroid|             Takeoff|    Tamil Nadu|  Chennai|      112|     2136|             1|    18| null|    294|             2|\n",
      "|  338585|      Manishearth|           @mozilla |   Maharashtra|   Mumbai|      459|     2008|           116|   714|   91|  11765|          4401|\n",
      "|10007129|     ironmaniiith|https://www.youtu...|Andhra Pradesh|Hyderabad|       47|     1989|             3|    44|    2|  43279|           322|\n",
      "| 1942872|    varshylmobile|Varshyl Mobile Pv...|          null|     null|      138|     1977|            21|     1|    1|    153|          null|\n",
      "| 3692893|          ritz078|         Housing.com|   Maharashtra|   Mumbai|       91|     1966|             9|   148|    4|   2543|           271|\n",
      "| 7391618|          sharish|                null|    Tamil Nadu|  Chennai|       64|     1965|          null|     1| null|     65|            19|\n",
      "| 3097802|        bitshadow|                null|     Karnataka|Bengaluru|       69|     1940|             4|    13|    1|    340|           104|\n",
      "| 3475919|       debugger22|  BITS Pilani, India|   Maharashtra|   Mumbai|      264|     1916|            15|   105|    4|   1465|           590|\n",
      "| 2416560|         titu1994|                null|          null|     null|       57|     1903|            17|     7| null|   1649|           198|\n",
      "|  269639|         kirang89|     WalletKit, Inc.|    Tamil Nadu|     null|      288|     1874|            22|    34|    3|    973|            59|\n",
      "|  956749|           mayuur|                null|       Gujarat|Ahmadabad|       88|     1861|             5|     4| null|    103|             1|\n",
      "| 4612622|     ManrajGrover|Practo Technologi...|     Karnataka|Bengaluru|      181|     1831|            20|   214|    3|   2370|           218|\n",
      "| 1305514|            jpuri|              Humble|         Delhi|New Delhi|       33|     1761|            13|   104|    2|   2569|          1110|\n",
      "| 3254200|        arvindr21|    The IoT Suitcase|Andhra Pradesh|Hyderabad|      669|     1757|            78|    37|    1|   1290|            86|\n",
      "|  972811|         amitkaps|            amitkaps|     Karnataka|Bengaluru|       64|     1745|            12|     7| null|   1110|            73|\n",
      "| 5958931|        vipulasri|                null|         Delhi|New Delhi|       55|     1691|             2|    73| null|    142|            22|\n",
      "| 3889952|       ganapativs|              Tracxn|     Karnataka|Bengaluru|       14|     1671|             6|    14| null|   1159|            53|\n",
      "+--------+-----------------+--------------------+--------------+---------+---------+---------+--------------+------+-----+-------+--------------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_more = utils.read_csv(spark, \"hdfs:/users_more.csv\")\n",
    "users = utils.read_csv(spark, \"hdfs:/users.csv\")\n",
    "\n",
    "users = (\n",
    "    users\n",
    "    .where(\n",
    "          (users.type == \"USR\")\n",
    "        & (users.deleted == 0)\n",
    "        & (users.fake == 0)\n",
    "        & (users.country_code == \"in\")\n",
    "    )\n",
    "    .withColumnRenamed(\"id\", \"user_id\")\n",
    "    .select(\"user_id\", \"login\", \"company\", \"state\", \"city\")\n",
    ")\n",
    "\n",
    "users_more = (\n",
    "    users_more\n",
    "    .select(\"user_id\", \"followers\", \"has_stars\", \"contributes_to\", \"issues\", \"pulls\", \"commits\", \"commits_others\")\n",
    ")\n",
    "\n",
    "user_new = (\n",
    "    users\n",
    "    .join(users_more, \"user_id\", \"left\")\n",
    ")\n",
    "\n",
    "top = (\n",
    "    user_new\n",
    "#     .orderBy(\"followers\", ascending=False)\n",
    "    .orderBy(\"has_stars\", ascending=False)\n",
    "#     .orderBy(\"contributes_to\", ascending=False)\n",
    ")\n",
    "\n",
    "top.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_more = utils.read_csv(spark, \"hdfs:/users_more.csv\")\n",
    "users = utils.read_csv(spark, \"hdfs:/users.csv\")\n",
    "\n",
    "users = (\n",
    "    users\n",
    "    .withColumnRenamed(\"id\", \"user_id\")\n",
    "#     .select(\"user_id\", \"login\", \"company\", \"state\", \"city\")\n",
    ")\n",
    "\n",
    "users_more = (\n",
    "    users_more\n",
    "#     .select(\"user_id\", \"followers\", \"has_stars\", \"contributes_to\", \"issues\", \"pulls\", \"commits\", \"commits_others\")\n",
    ")\n",
    "\n",
    "res = (\n",
    "    users\n",
    "    .join(users_more, \"user_id\", \"left\")\n",
    ")\n",
    "\n",
    "res.write.csv(\n",
    "    \"hdfs:/users_with_more\",\n",
    "    nullValue=\"\\\\N\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commits Punchcard for top users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "commits = utils.read_csv(spark, \"hdfs:/commits_new.csv\")\n",
    "projects = utils.read_csv(spark, \"hdfs:/projects_new.csv\")\n",
    "\n",
    "# top_users = [5203, 896, 376498, 6240, 1779, 9236, 1570, 3871, 1736, 13009, 24452, 616741, 2468643, 2427, 81423, 796, 10005, 417948, 2016667, 1954]\n",
    "jamians = [1432224, 5107602, 4007006, 6145009, 2859386, 4925305, 2549876]\n",
    "\n",
    "res = (\n",
    "    commits\n",
    "    .where(\n",
    "        (commits.created_at.isNotNull())\n",
    "#         & (commits.author_id.isin(top_users))\n",
    "        & (commits.author_id.isin(jamians))\n",
    "    )\n",
    "    .select(\n",
    "        \"author_id\",\n",
    "        F.date_format('created_at', 'E').alias('day'),\n",
    "        F.hour('created_at').alias('hour')\n",
    "    )\n",
    "   .groupBy(\n",
    "    'author_id',\n",
    "    'day',\n",
    "    'hour'\n",
    "   )\n",
    "   .count()\n",
    "   .withColumnRenamed(\"count\", \"commits\")\n",
    "#    .withColumnRenamed(\"author_id\", \"user_id\")\n",
    ")\n",
    "\n",
    "res.write.json(\n",
    "    \"hdfs:/jamians_commit_punchcard\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commits Punchcard for EVERYONE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = (\n",
    "    commits\n",
    "    .where(\n",
    "        (commits.created_at.isNotNull())\n",
    "    )\n",
    "    .select(\n",
    "        F.date_format('created_at', 'E').alias('day'),\n",
    "        F.hour('created_at').alias('hour')\n",
    "    )\n",
    "   .groupBy(\n",
    "    'day',\n",
    "    'hour'\n",
    "   )\n",
    "   .count()\n",
    "   .withColumnRenamed(\"count\", \"commits\")\n",
    ")\n",
    "\n",
    "res.write.json(\n",
    "    \"hdfs:/commit_punchcard\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projects (More Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects =  utils.read_csv(spark, \"hdfs:/projects_new.csv\")\n",
    "stars = utils.read_csv(spark, \"hdfs:/watchers.csv\")\n",
    "issues = utils.read_csv(spark, \"hdfs:/issues.csv\")\n",
    "\n",
    "pcommits = utils.read_csv(spark, \"hdfs:/project_commit_count.csv\")\n",
    "pcommits_others = utils.read_csv(spark, \"hdfs:/project_commit_others_count.csv\")\n",
    "\n",
    "members = utils.read_csv(spark, \"hdfs:/project_members_new.csv\")\n",
    "\n",
    "# Find stars on a repo\n",
    "pstars = (\n",
    "    stars\n",
    "    .groupby(\"repo_id\")\n",
    "    .count()\n",
    "    .withColumnRenamed(\"count\", \"stars\")\n",
    "    .withColumnRenamed(\"repo_id\", \"project_id\")\n",
    ")\n",
    "\n",
    "# Find forks of a repo\n",
    "pforks = (\n",
    "    projects\n",
    "    .where(projects.forked_from.isNotNull())\n",
    "    .groupby(\"forked_from\")\n",
    "    .count()\n",
    "    .withColumnRenamed(\"count\", \"forks\")\n",
    "    .withColumnRenamed(\"forked_from\", \"project_id\")\n",
    ")\n",
    "\n",
    "# Find total issues on a repo\n",
    "pissues = (\n",
    "    issues\n",
    "    .where(issues.pull_request_id.isNull())\n",
    "    .groupby(\"repo_id\")\n",
    "    .count()\n",
    "    .withColumnRenamed(\"count\", \"issues\")\n",
    "    .withColumnRenamed(\"repo_id\", \"project_id\")\n",
    ")\n",
    "\n",
    "# Find pull requests on a repo\n",
    "ppullreqs = (\n",
    "    issues\n",
    "    .where(issues.pull_request_id.isNotNull())\n",
    "    .groupby(\"repo_id\")\n",
    "    .count()\n",
    "    .withColumnRenamed(\"count\", \"pull_requests\")\n",
    "    .withColumnRenamed(\"repo_id\", \"project_id\")\n",
    ")\n",
    "\n",
    "# Find members of a repo\n",
    "pmembers = (\n",
    "    members\n",
    "    .groupby(\"project_id\")\n",
    "    .count()\n",
    "    .withColumnRenamed(\"count\", \"contributors\")\n",
    ")\n",
    "\n",
    "# Join Data\n",
    "pmore = (\n",
    "    pstars\n",
    "    .join(pforks, \"project_id\", \"full\")\n",
    "    .join(pmembers, \"project_id\", \"full\")\n",
    "    .join(pissues, \"project_id\", \"full\")\n",
    "    .join(ppullreqs, \"project_id\", \"full\")\n",
    "    .join(pcommits, \"project_id\", \"full\")\n",
    "    .join(pcommits_others, \"project_id\", \"full\")\n",
    ")\n",
    "\n",
    "# pmore.show()\n",
    "\n",
    "pmore.write.csv(\n",
    "    \"hdfs:/projects_more\",\n",
    "    mode=\"overwrite\",\n",
    "    nullValue=\"\\\\N\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Users (More Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues = utils.read_csv(spark, \"hdfs:/issues.csv\")\n",
    "members = utils.read_csv(spark, \"hdfs:/project_members_new.csv\")\n",
    "\n",
    "user_more = utils.read_csv(spark, \"hdfs:/user_more.csv\")\n",
    "\n",
    "ucommits = utils.read_csv(spark, \"hdfs:/user_commit_count.csv\")\n",
    "ucommits_others = utils.read_csv(spark, \"hdfs:/user_commit_others_count.csv\")\n",
    "\n",
    "projects = utils.read_csv(spark, \"hdfs:/projects_new.csv\")\n",
    "projects_more = utils.read_csv(spark, \"hdfs:/projects_more\")\n",
    "\n",
    "projects.createOrReplaceTempView(\"projects\")\n",
    "projects_more.createOrReplaceTempView(\"projects_more\")\n",
    "\n",
    "# Find total stars that a users repos have recieved\n",
    "q = \"\"\"\n",
    "    SELECT P.owner_id as user_id, sum(M.stars) as stars_on_repos\n",
    "    FROM projects as P, projects_more as M\n",
    "    WHERE P.id = M.project_id\n",
    "    GROUP BY P.owner_id\n",
    "\"\"\"\n",
    "\n",
    "ustars = spark.sql(q)\n",
    "\n",
    "# Find total repos a user is members of\n",
    "umembers = (\n",
    "    members\n",
    "    .groupby(\"user_id\")\n",
    "    .count()\n",
    "    .withColumnRenamed(\"count\", \"members_of_repos\")\n",
    ")\n",
    "\n",
    "\n",
    "# Find total issues by user\n",
    "uissues = (\n",
    "    issues\n",
    "    .where(issues.pull_request_id.isNull())\n",
    "    .groupby(\"reporter_id\")\n",
    "    .count()\n",
    "    .withColumnRenamed(\"count\", \"issues\")\n",
    "    .withColumnRenamed(\"reporter_id\", \"user_id\")\n",
    ")\n",
    "\n",
    "# Find pull requests by user\n",
    "upullreqs = (\n",
    "    issues\n",
    "    .where(issues.pull_request_id.isNotNull())\n",
    "    .groupby(\"reporter_id\")\n",
    "    .count()\n",
    "    .withColumnRenamed(\"count\", \"pull_requests\")\n",
    "    .withColumnRenamed(\"reporter_id\", \"user_id\")\n",
    ")\n",
    "\n",
    "# Join Data\n",
    "umore = (\n",
    "    user_more\n",
    "    .join(ustars, \"user_id\", \"full\")\n",
    "    .join(umembers, \"user_id\", \"full\")\n",
    "    .join(uissues, \"user_id\", \"full\")\n",
    "    .join(upullreqs, \"user_id\", \"full\")\n",
    "    .join(ucommits, \"user_id\", \"full\")\n",
    "    .join(ucommits_others, \"user_id\", \"full\")\n",
    ")\n",
    "\n",
    "umore.write.csv(\n",
    "    \"hdfs:/users_most\",\n",
    "    mode=\"overwrite\",\n",
    "    nullValue=\"\\\\N\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stars, Followers of Top Corporates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+---------+-----------+---------------+------------------+----------------------+\n",
      "|corporate                     |employees|total_stars|total_followers|stars_per_employee|followers_per_employee|\n",
      "+------------------------------+---------+-----------+---------------+------------------+----------------------+\n",
      "|Red Hat                       |69       |1690       |1058           |41                |25                    |\n",
      "|Flipkart                      |156      |1078       |582            |27                |11                    |\n",
      "|ThoughtWorks                  |163      |2536       |1018           |32                |10                    |\n",
      "|Microsoft                     |235      |892        |1102           |14                |10                    |\n",
      "|IBM                           |157      |1117       |431            |33                |9                     |\n",
      "|Practo                        |82       |318        |209            |14                |6                     |\n",
      "|Thoughtworks                  |67       |266        |202            |9                 |6                     |\n",
      "|Freshdesk                     |101      |355        |255            |14                |6                     |\n",
      "|Amazon                        |73       |274        |234            |9                 |6                     |\n",
      "|Sapient                       |120      |74         |119            |6                 |5                     |\n",
      "|Tata Consultancy Services     |173      |371        |189            |13                |5                     |\n",
      "|Oracle                        |108      |75         |150            |3                 |4                     |\n",
      "|Cognizant Technology Solutions|166      |406        |212            |11                |4                     |\n",
      "|Tech Mahindra                 |73       |91         |65             |6                 |4                     |\n",
      "|TCS                           |261      |217        |186            |7                 |4                     |\n",
      "|Cognizant                     |132      |144        |141            |6                 |4                     |\n",
      "|Infosys                       |164      |88         |95             |4                 |3                     |\n",
      "|Accenture                     |184      |111        |99             |5                 |3                     |\n",
      "|Wipro Technologies            |84       |71         |57             |5                 |3                     |\n",
      "|Capgemini                     |113      |141        |83             |11                |3                     |\n",
      "+------------------------------+---------+-----------+---------------+------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "top_orgs = {\n",
    "    \"Microsoft\": 8071,\n",
    "    \"IBM\": 2804,\n",
    "    \"Google\": 2725,\n",
    "    \"Red Hat\": 1669,\n",
    "    \"Intel\": 1027,\n",
    "    \"ThoughtWorks\": 937,\n",
    "    \"Facebook\": 925,\n",
    "    \"Esri\": 888,\n",
    "    \"Tencent\": 818,\n",
    "    \"Oracle\": 672,\n",
    "    \"Accenture\": 656,\n",
    "    \"Baidu\": 552,\n",
    "    \"Amazon\": 549,\n",
    "    \"EPAM Systems\": 519,\n",
    "    \"Yandex\": 490,\n",
    "    \"Capgemini\": 488,\n",
    "    \"TCS\": 475,\n",
    "    \"Cisco\": 470,\n",
    "    \"Alibaba\": 461,\n",
    "    \"Mozilla\": 457,\n",
    "\n",
    "}\n",
    "\n",
    "top_orgs_indian = {\n",
    "    \"TCS\": 261,\n",
    "    \"Microsoft\": 227,\n",
    "    \"Accenture\": 181,\n",
    "    \"Tata Consultancy Services\": 169,\n",
    "    \"ThoughtWorks\": 163,\n",
    "    \"Cognizant Technology Solutions\": 163,\n",
    "    \"Infosys\": 162,\n",
    "    \"Flipkart\": 154,\n",
    "    \"IBM\": 149,\n",
    "    \"Cognizant\": 125,\n",
    "    \"Sapient\": 119,\n",
    "    \"Capgemini\": 109,\n",
    "    \"Freshdesk\": 100,\n",
    "    \"Oracle\": 99,\n",
    "    \"Practo\": 80,\n",
    "    \"Wipro Technologies\": 79,\n",
    "    \"Tech Mahindra\": 72,\n",
    "    \"Amazon\": 71,\n",
    "    \"Red Hat\": 68,\n",
    "    \"Thoughtworks\": 66,\n",
    "}\n",
    "\n",
    "# top_corps = list(top_orgs.keys())\n",
    "top_corps = list(top_orgs_indian.keys())\n",
    "\n",
    "MSFT = [\"Microsoft Corporation\", \"Microsoft\"]\n",
    "\n",
    "user_more = utils.read_csv(spark, \"hdfs:/users_with_more.csv\")\n",
    "\n",
    "users_of_top = (\n",
    "    user_more\n",
    "    \n",
    "    .where(user_more.country_code == \"in\")\n",
    "    \n",
    "    .select(\"company\", \"has_stars\", \"followers\",\n",
    "            F.when(user_more.company.isin(MSFT), \"Microsoft\")\n",
    "             .otherwise(user_more.company).name(\"corporate\")\n",
    "    )\n",
    ")\n",
    "\n",
    "users_of_top = (\n",
    "    users_of_top\n",
    "    .where(users_of_top.corporate.isin(top_corps))\n",
    ")\n",
    "\n",
    "users_of_top.createOrReplaceTempView(\"users_top\")\n",
    "\n",
    "q = \"\"\"\n",
    "\n",
    "    SELECT corporate, count(*) as employees, \n",
    "           sum(`has_stars`) as total_stars, \n",
    "           sum(`followers`) as total_followers, \n",
    "           ceiling(avg(has_stars)) as stars_per_employee, \n",
    "           ceiling(avg(followers)) as followers_per_employee\n",
    "           \n",
    "    FROM users_top as ut\n",
    "    GROUP BY corporate\n",
    "    \n",
    "    ORDER BY followers_per_employee DESC\n",
    "\"\"\"\n",
    "\n",
    "res = spark.sql(q)\n",
    "\n",
    "res.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### %age Community Particitpation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+\n",
      "|community_part|num_repos|\n",
      "+--------------+---------+\n",
      "|             0| 51332178|\n",
      "|             1|    43473|\n",
      "|             2|    75228|\n",
      "|             3|    78215|\n",
      "|             4|    85897|\n",
      "|             5|    82493|\n",
      "|             6|    73771|\n",
      "|             7|    70892|\n",
      "|             8|    80642|\n",
      "|             9|    58070|\n",
      "|            10|   108058|\n",
      "|            11|    28597|\n",
      "|            12|    82883|\n",
      "|            13|    79445|\n",
      "|            14|    37588|\n",
      "|            15|    98982|\n",
      "|            16|    38888|\n",
      "|            17|   114522|\n",
      "|            18|    26443|\n",
      "|            19|    44038|\n",
      "|            20|   159205|\n",
      "|            21|    14105|\n",
      "|            22|    30787|\n",
      "|            23|    49053|\n",
      "|            24|    35213|\n",
      "|            25|   222877|\n",
      "|            26|    11828|\n",
      "|            27|    26534|\n",
      "|            28|    35727|\n",
      "|            29|    62526|\n",
      "|            30|    41438|\n",
      "|            31|    23168|\n",
      "|            32|    26018|\n",
      "|            33|    12930|\n",
      "|            34|   356358|\n",
      "|            35|    20192|\n",
      "|            36|    25772|\n",
      "|            37|    29525|\n",
      "|            38|    45314|\n",
      "|            39|    27286|\n",
      "|            40|   113932|\n",
      "|            41|    13294|\n",
      "|            42|    28368|\n",
      "|            43|    57700|\n",
      "|            44|    20831|\n",
      "|            45|    37520|\n",
      "|            46|    26954|\n",
      "|            47|    26997|\n",
      "|            48|    24817|\n",
      "|            49|    14910|\n",
      "|            50|   874552|\n",
      "|            51|     5764|\n",
      "|            52|    16580|\n",
      "|            53|    21913|\n",
      "|            54|    26343|\n",
      "|            55|    29124|\n",
      "|            56|    33582|\n",
      "|            57|    17253|\n",
      "|            58|    53747|\n",
      "|            59|    25407|\n",
      "|            60|   105616|\n",
      "|            61|    11555|\n",
      "|            62|    24208|\n",
      "|            63|    39449|\n",
      "|            64|    26762|\n",
      "|            65|    22980|\n",
      "|            66|    13784|\n",
      "|            67|   415695|\n",
      "|            68|    12558|\n",
      "|            69|    19112|\n",
      "|            70|    35840|\n",
      "|            71|    15293|\n",
      "|            72|    52905|\n",
      "|            73|    25507|\n",
      "|            74|    19669|\n",
      "|            75|   221661|\n",
      "|            76|     9751|\n",
      "|            77|    23610|\n",
      "|            78|    34937|\n",
      "|            79|    20801|\n",
      "|            80|   145942|\n",
      "|            81|    11416|\n",
      "|            82|    29728|\n",
      "|            83|    16654|\n",
      "|            84|   103803|\n",
      "|            85|    25904|\n",
      "|            86|    78120|\n",
      "|            87|    22005|\n",
      "|            88|    65801|\n",
      "|            89|    58046|\n",
      "|            90|    49544|\n",
      "|            91|    41911|\n",
      "|            92|    39473|\n",
      "|            93|    48456|\n",
      "|            94|    41762|\n",
      "|            95|    50524|\n",
      "|            96|    44361|\n",
      "|            97|    39478|\n",
      "|            98|    38937|\n",
      "|            99|    33912|\n",
      "+--------------+---------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pm = utils.read_csv(spark, \"hdfs:/projects_more.csv\")\n",
    "proj = utils.read_csv(spark, \"hdfs:/projects_new.csv\")\n",
    "\n",
    "pm = (\n",
    "#     pm\n",
    "    proj.join(pm, proj.id == pm.project_id, \"left\")\n",
    "    .na.fill({'commits': 1, 'commits_by_others': 0})\n",
    ")\n",
    "\n",
    "pm.createOrReplaceTempView(\"projects_more\")\n",
    "\n",
    "q = \"\"\"\n",
    "    SELECT ceiling( 100 * (commits_by_others) / commits ) as community_part, count(*) as num_repos\n",
    "    FROM projects_more\n",
    "    GROUP BY community_part\n",
    "    ORDER BY community_part\n",
    "\"\"\"\n",
    "\n",
    "# pm.limit(50).show(50)\n",
    "\n",
    "res = spark.sql(q)\n",
    "res.show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25922057"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pm = utils.read_csv(spark, \"hdfs:/projects_more.csv\")\n",
    "proj = utils.read_csv(spark, \"hdfs:/projects_new.csv\")\n",
    "\n",
    "pm = (\n",
    "    proj.join(pm, proj.id == pm.project_id, \"left\")\n",
    "    .na.fill({'commits': 0, 'commits_by_others': 0})\n",
    ")\n",
    "\n",
    "pm.where(pm.commits == 0).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lifespan of Projects (by last-first commit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = utils.read_csv(spark, \"hdfs:/commits_new.csv\")\n",
    "cm.createOrReplaceTempView(\"commits\")\n",
    "\n",
    "q = \"\"\"\n",
    "    SELECT project_id,\n",
    "           min(created_at) as first,\n",
    "           max(created_at) as last\n",
    "    FROM commits\n",
    "    WHERE\n",
    "      created_at < CURRENT_TIMESTAMP\n",
    "      AND project_id IS NOT NULL\n",
    "    GROUP BY project_id\n",
    "\"\"\"\n",
    "\n",
    "res = spark.sql(q)\n",
    "\n",
    "res = (\n",
    "    res\n",
    "    .withColumn('duration', F.datediff(F.col(\"last\"), F.col(\"first\")))\n",
    "    .select(\"project_id\", \"duration\")\n",
    "    .groupby(\"duration\")\n",
    "    .count()\n",
    "    .sort(F.desc(\"duration\"))\n",
    ")\n",
    "\n",
    "res.write.json(\n",
    "    \"hdfs:/lifespan_commits_1.json\",\n",
    "    mode=\"overwrite\",\n",
    "#     nullValue=\"\\\\N\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----+----------+\n",
      "|project_id|    language|year|max(bytes)|\n",
      "+----------+------------+----+----------+\n",
      "|  13496571|           d|2015|      3759|\n",
      "|   5999634|        perl|2015|    465114|\n",
      "|  11309521|      python|2015|      7790|\n",
      "|  16054889|       shell|2015|       478|\n",
      "|  22350750|         css|2015|       808|\n",
      "|  15363845|      prolog|2015|       199|\n",
      "|   6030637|       shell|2015|        64|\n",
      "|  10750668|         c++|2015|     33000|\n",
      "|  25330477|         css|2015|       122|\n",
      "|   4650978|        java|2015|    140600|\n",
      "|  19559892|         css|2015|      1717|\n",
      "|  18461008|        xslt|2015|    220483|\n",
      "|  21784276|         css|2015|      1934|\n",
      "|  21949925|        viml|2015|     16714|\n",
      "|   2529417|coffeescript|2015|     45230|\n",
      "|  16543457|        haxe|2015|     11075|\n",
      "|  19814407|      python|2015|    479806|\n",
      "|  10522756|       shell|2015|       126|\n",
      "|  14547901|        java|2015|      2507|\n",
      "|  14548067|  javascript|2015|      3386|\n",
      "+----------+------------+----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "'Cannot resolve column name \"bytes\" among (project_id, language, year, max(bytes));'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o2000.sum.\n: org.apache.spark.sql.AnalysisException: Cannot resolve column name \"bytes\" among (project_id, language, year, max(bytes));\n\tat org.apache.spark.sql.Dataset$$anonfun$resolve$1.apply(Dataset.scala:216)\n\tat org.apache.spark.sql.Dataset$$anonfun$resolve$1.apply(Dataset.scala:216)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.sql.Dataset.resolve(Dataset.scala:215)\n\tat org.apache.spark.sql.RelationalGroupedDataset$$anonfun$3.apply(RelationalGroupedDataset.scala:99)\n\tat org.apache.spark.sql.RelationalGroupedDataset$$anonfun$3.apply(RelationalGroupedDataset.scala:98)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n\tat scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:54)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.RelationalGroupedDataset.aggregateNumericColumns(RelationalGroupedDataset.scala:98)\n\tat org.apache.spark.sql.RelationalGroupedDataset.sum(RelationalGroupedDataset.scala:296)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-8a89743af37c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mres1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"language\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"year\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bytes\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bytes\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m )\n",
      "\u001b[0;32m~/spark/python/pyspark/sql/group.py\u001b[0m in \u001b[0;36m_api\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jgd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Cannot resolve column name \"bytes\" among (project_id, language, year, max(bytes));'"
     ]
    }
   ],
   "source": [
    "    \n",
    "pl = utils.read_csv(spark, \"hdfs:/project_languages.csv\")\n",
    "\n",
    "res1 = (\n",
    "    pl\n",
    "    .select(\"project_id\", \"language\", \"bytes\", F.year(\"created_at\").alias(\"year\"))\n",
    "    .groupby(\"project_id\", \"language\", \"year\")\n",
    "    .max(\"bytes\")\n",
    ")\n",
    "\n",
    "res1.show(20)\n",
    "\n",
    "res2 = (\n",
    "    res1\n",
    "    .groupby(\"language\", \"year\")\n",
    "    .sum(\"bytes\")\n",
    "    .sort(\"bytes\", False)\n",
    ")\n",
    "\n",
    "res2.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
