{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.types as spark_types\n",
    "\n",
    "import utils\n",
    "\n",
    "spark = SparkSession.builder.master(\"spark://vm1:7077\").appName(\"Cluster Code - Zafar\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing to see if everything is okay\n",
    "commits = utils.read_csv(spark, \"hdfs:/commits_new.csv\")\n",
    "commits.limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of source & fork repositories of users (shouldn't be run again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects =  utils.read_csv(spark, \"hdfs:/projects.csv\", \"projects_new.csv\")\n",
    "\n",
    "# This file has now been replaced with a new version\n",
    "user_more =  utils.read_csv(spark, \"hdfs:/user_more.csv\")\n",
    "\n",
    "# Find source repos\n",
    "df4 = (\n",
    "    projects\n",
    "    .where((projects.deleted == 0) & (projects.forked_from.isNull()))\n",
    "    .groupby(\"owner_id\")\n",
    "    .count()\n",
    "    .withColumnRenamed(\"count\", \"repos_source\")\n",
    "    .withColumnRenamed(\"owner_id\", \"user_id\")\n",
    ")\n",
    "\n",
    "# Find forks\n",
    "df6 = (\n",
    "    projects\n",
    "    .where((projects.deleted == 0) & (projects.forked_from.isNotNull()))\n",
    "    .groupby(\"owner_id\")\n",
    "    .count()\n",
    "    .withColumnRenamed(\"count\", \"repos_forks\")\n",
    "    .withColumnRenamed(\"owner_id\", \"user_id\")\n",
    ")\n",
    "\n",
    "# Join Data\n",
    "df5 = user_more.join(df4, \"user_id\", \"full\").join(df6, \"user_id\", \"full\")\n",
    "\n",
    "# Write to local directorya\n",
    "df5.write.csv(\n",
    "    \"/user_more_2\",\n",
    "    mode=\"overwrite\",\n",
    "    nullValue=\"\\\\N\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue Punchcard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+----+--------+\n",
      "|year|month| day|hour|   count|\n",
      "+----+-----+----+----+--------+\n",
      "|null| null|null|null|54086297|\n",
      "+----+-----+----+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "issues = utils.read_csv(spark, \"hdfs:/issues.csv\")\n",
    "\n",
    "df5 = (\n",
    "    issues\n",
    "    .where(\n",
    "        issues.created_at.isNotNull()\n",
    "    )\n",
    "    .select(\n",
    "        F.year('created_at').alias('year'), \n",
    "        F.month('created_at').alias('month'), \n",
    "        F.dayofmonth('created_at').alias('day'), \n",
    "        F.hour('created_at').alias('hour')\n",
    "    )\n",
    "   .groupBy('year', 'month', 'day', 'hour')\n",
    "   .count()\n",
    ")\n",
    "\n",
    "df5.limit(10).show()\n",
    "\n",
    "# df5.explain()\n",
    "# df5.coalesce(1).write.json(\"hdfs:/issue_punchcard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Commits of each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "commits = utils.read_csv(spark, \"hdfs:/commits_new.csv\")\n",
    "\n",
    "res = (\n",
    "    commits\n",
    "    .groupby(\"author_id\")\n",
    "    .count()\n",
    "    .withColumnRenamed(\"count\", \"commits_authored\")\n",
    "    .withColumnRenamed(\"author_id\", \"user_id\")\n",
    ")\n",
    "\n",
    "res.write.csv(\n",
    "    \"hdfs:/user_commit_count\",\n",
    "    mode=\"overwrite\",\n",
    "    nullValue=\"\\\\N\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Commits of every project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# commits = utils.read_csv(spark, \"hdfs:/commits_new.csv\")\n",
    "\n",
    "res = (\n",
    "    commits\n",
    "    .groupby(\"project_id\")\n",
    "    .count()\n",
    "    .withColumnRenamed(\"count\", \"total_commits\")\n",
    ")\n",
    "\n",
    "res.write.csv(\n",
    "    \"hdfs:/project_commit_count\",\n",
    "#     mode=\"overwrite\",\n",
    "    nullValue=\"\\\\N\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Commits on a project not authored by Owner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "commits = utils.read_csv(spark, \"hdfs:/commits_new.csv\")\n",
    "projects = utils.read_csv(spark, \"hdfs:/projects_new.csv\")\n",
    "\n",
    "commits.createOrReplaceTempView(\"commits\")\n",
    "projects.createOrReplaceTempView(\"projects\")\n",
    "\n",
    "q = \"\"\"\n",
    "    SELECT C.project_id as project_id, COUNT(*) as total_commits_by_others\n",
    "    FROM commits as C, projects as P\n",
    "    WHERE C.project_id = P.id\n",
    "    AND C.author_id <> P.owner_id\n",
    "    GROUP BY C.project_id\n",
    "\"\"\"\n",
    "\n",
    "res = spark.sql(q)\n",
    "\n",
    "res.write.csv(\n",
    "    \"hdfs:/project_commit_others_count\",\n",
    "#     mode=\"overwrite\",\n",
    "    nullValue=\"\\\\N\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of commits of a user made on repositories not owned by them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"\"\"\n",
    "    SELECT C.author_id as user_id, \n",
    "           COUNT(*) as total_commits_on_other_repos\n",
    "\n",
    "    FROM commits as C, projects as P\n",
    "\n",
    "    WHERE C.project_id = P.id\n",
    "    AND C.author_id <> P.owner_id\n",
    "\n",
    "    GROUP BY C.author_id\n",
    "\"\"\"\n",
    "\n",
    "res = spark.sql(q)\n",
    "\n",
    "res.write.csv(\n",
    "    \"hdfs:/user_commit_others_count\",\n",
    "#     mode=\"overwrite\",\n",
    "    nullValue=\"\\\\N\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Top\" Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+--------------------+----+----+-------+---------+---------+-------+------------+-----------+\n",
      "|user_id|           login|             company|type|fake|deleted|following|followers|starred|repos_source|repos_forks|\n",
      "+-------+----------------+--------------------+----+----+-------+---------+---------+-------+------------+-----------+\n",
      "|   5203|        torvalds|    Linux Foundation| USR|   0|      0|     null|    52722|      1|           5|          2|\n",
      "|    896|     JakeWharton|        Square, Inc.| USR|   0|      0|       24|    30161|    261|          86|         24|\n",
      "| 376498|              Tj|                Apex| USR|   0|      0|      175|    25827|    966|         238|         81|\n",
      "|   6240|      addyosmani|              Google| USR|   0|      0|      243|    24604|    522|         147|        154|\n",
      "|   1779|       paulirish|   Google Chrome, â™¥z| USR|   0|      0|      253|    24510|    445|          56|        242|\n",
      "|   9236|         mojombo|                null| USR|   0|      0|       11|    23076|     37|          40|         20|\n",
      "|   1570|         defunkt|            @github | USR|   0|      0|      230|    18522|    139|          78|         29|\n",
      "|   3871|    sindresorhus|@avajs @chalk @ye...| USR|   0|      0|      188|    15755|   2274|         974|        142|\n",
      "|   1736|douglascrockford|                null| USR|   0|      0|     null|    15396|      5|          16|       null|\n",
      "|  13009|        mbostock|                null| USR|   0|      0|       13|    14928|     29|          53|         18|\n",
      "|  24452|         jeresig|              @Khan | USR|   0|      0|       33|    14703|    372|          48|         47|\n",
      "| 616741|          ruanyf|                null| USR|   0|      0|     null|    14701|    124|          36|         12|\n",
      "|2468643|        daimajia|Beijing Normal Un...| USR|   0|      0|      234|    14507|   1974|          24|         37|\n",
      "|   2427|           mattt|              @apple| USR|   0|      0|      203|    13705|    293|          59|          2|\n",
      "|  81423|             mdo|              GitHub| USR|   0|      0|       23|    13443|    153|          23|         17|\n",
      "|    796|    kennethreitz|              Heroku| USR|   0|      0|      224|    13360|    919|         119|         37|\n",
      "|  10005|         schacon|                null| USR|   0|      0|       24|    13102|     86|          94|         82|\n",
      "| 417948|         gaearon|            Facebook| USR|   0|      0|      177|    13007|   1172|          59|        177|\n",
      "|2016667|           jlord|              GitHub| USR|   0|      0|       32|    11897|    139|         109|         41|\n",
      "|   1954|     visionmedia|                null| USR|   0|      0|      157|    11714|    624|         160|         63|\n",
      "+-------+----------------+--------------------+----+----+-------+---------+---------+-------+------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_more = utils.read_csv(spark, \"hdfs:/user_more.csv\")\n",
    "users = utils.read_csv(spark, \"hdfs:/users.csv\")\n",
    "\n",
    "users = (\n",
    "    users\n",
    "    .withColumnRenamed(\"id\", \"user_id\")\n",
    "    .select(\"user_id\", \"login\", \"company\", \"type\", \"fake\", \"deleted\")\n",
    ")\n",
    "\n",
    "user_new = users.join(user_more, \"user_id\", \"left\")\n",
    "\n",
    "# Torvalds is at the top\n",
    "# most_followers = user_new.orderBy(\"followers\", ascending=False)\n",
    "\n",
    "# most_followers.show()\n",
    "top = (\n",
    "    user_new\n",
    "    .where(\n",
    "        (user_new.type == \"USR\")\n",
    "        & (user_new.followers.isNotNull())\n",
    "#         & (user_new.starred.isNotNull())\n",
    "    )\n",
    "    .orderBy(\"followers\", ascending=False)\n",
    ")\n",
    "\n",
    "top.show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commits Punchcard for top users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "commits = utils.read_csv(spark, \"hdfs:/commits_new.csv\")\n",
    "projects = utils.read_csv(spark, \"hdfs:/projects_new.csv\")\n",
    "\n",
    "top_users = [5203, 896, 376498, 6240, 1779, 9236, 1570, 3871, 1736, 13009, 24452, 616741, 2468643, 2427, 81423, 796, 10005, 417948, 2016667, 1954]\n",
    "\n",
    "res = (\n",
    "    commits\n",
    "    .where(\n",
    "        (commits.created_at.isNotNull())\n",
    "        & (commits.author_id.isin(top_users))\n",
    "    )\n",
    "    .select(\n",
    "        \"author_id\",\n",
    "        F.date_format('created_at', 'E').alias('day'),\n",
    "        F.hour('created_at').alias('hour')\n",
    "    )\n",
    "   .groupBy(\n",
    "    'author_id',\n",
    "    'day',\n",
    "    'hour'\n",
    "   )\n",
    "   .count()\n",
    "   .withColumnRenamed(\"count\", \"commits\")\n",
    "#    .withColumnRenamed(\"author_id\", \"user_id\")\n",
    ")\n",
    "\n",
    "res.write.json(\n",
    "    \"hdfs:/top_users_commit_punchcard\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commits Punchcard for EVERYONE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = (\n",
    "    commits\n",
    "    .where(\n",
    "        (commits.created_at.isNotNull())\n",
    "    )\n",
    "    .select(\n",
    "        F.date_format('created_at', 'E').alias('day'),\n",
    "        F.hour('created_at').alias('hour')\n",
    "    )\n",
    "   .groupBy(\n",
    "    'day',\n",
    "    'hour'\n",
    "   )\n",
    "   .count()\n",
    "   .withColumnRenamed(\"count\", \"commits\")\n",
    ")\n",
    "\n",
    "res.write.json(\n",
    "    \"hdfs:/commit_punchcard\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
